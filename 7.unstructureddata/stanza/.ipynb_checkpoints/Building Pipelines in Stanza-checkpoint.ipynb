{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stanza</b>\n",
    "<br>\n",
    "Stanza provides simple, flexible, and unified interfaces for downloading and running various NLP models, you can refer to the Downloading Models and Pipeline pages. At a high level, to start annotating text, firstly, you need to initialize a Pipeline, which pre-loads and chains up a series of Processors, with each processor performing a specific NLP task (e.g., tokenization, dependency parsing, or named entity recognition). \n",
    "<br>\n",
    "<br>\n",
    "Literally saying, it is essential in most of the cases to download the pre-trained model language from Stanza before conducting further training with NLP tasks. Itâ€™s just simple with the stanza.download command. The language can be specified with either a full language name (e.g., \"Japanese\"), or a short code (e.g., \"ja\")\n",
    "<br>\n",
    "The reference paper for Stanza is available on this <a href=\"https://arxiv.org/abs/2003.07082\"> link </a>\n",
    "<br>\n",
    "In this course we are going to work with pre-trained language models. Of course, if you download the stanza code from the corresponding github page, you can start working on your own models. To create for instance your own named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Barack Obama was born in Hawaii. His style is different from Donald Trump's\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Tokenizer</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize',use_gpu=False, verbose=False, pos_batch_size=3000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text) # Run the pipeline on the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: Barack\n",
      "id: (2,)\ttext: Obama\n",
      "id: (3,)\ttext: was\n",
      "id: (4,)\ttext: born\n",
      "id: (5,)\ttext: in\n",
      "id: (6,)\ttext: Hawaii\n",
      "id: (7,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: His\n",
      "id: (2,)\ttext: style\n",
      "id: (3,)\ttext: is\n",
      "id: (4,)\ttext: different\n",
      "id: (5,)\ttext: from\n",
      "id: (6,)\ttext: Donald\n",
      "id: (7,)\ttext: Trump's\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also can perform tokenizing your text given existing full sentence `without segmentation`, one just needs to set `tokenize_no_ssplit` as `True` to disable sentence segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: (1,)\ttext: Barack\n",
      "id: (2,)\ttext: Obama\n",
      "id: (3,)\ttext: was\n",
      "id: (4,)\ttext: born\n",
      "id: (5,)\ttext: in\n",
      "id: (6,)\ttext: Hawaii\n",
      "id: (7,)\ttext: .\n",
      "id: (8,)\ttext: His\n",
      "id: (9,)\ttext: style\n",
      "id: (10,)\ttext: is\n",
      "id: (11,)\ttext: different\n",
      "id: (12,)\ttext: from\n",
      "id: (13,)\ttext: Donald\n",
      "id: (14,)\ttext: Trump's\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize',use_gpu=False, tokenize_no_ssplit=True,verbose=False, pos_batch_size=3000) # Build the pipeline, specify part-of-speech processor's batch size\n",
    "doc = nlp(text) # Run the pipeline on the input text\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already tokenized your text, and just want to use Stanza for downstream processing, setting `tokenize_pretokenized` as `True` to bypass the neural tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.', 'His', 'style', 'is', 'different', 'from', 'Donald', \"Trump's\"]\n"
     ]
    }
   ],
   "source": [
    "# from the previous text we already had:\n",
    "tokens = [i.text for i in sentence.tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.', 'His', 'style', 'is', 'different', 'from', 'Donald', \"Trump's\"]\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en',tokenize_pretokenized=True,verbose=False,processors='tokenize')\n",
    "doc= nlp(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part of Speech (POS)</b><br>\n",
    "Stanza also supplies a processor to label the token with their universal POS (`UPOS`) tags, treebank-specific POS (`XPOS`) tags, and universal morphological features (`UFeats`). \n",
    "<br>\n",
    "The part-of-speech tags can be accessed via the `upos`(pos) and `xpos` fields of each Word from the Sentences. \n",
    "\n",
    "<br>\n",
    "Note: POSProcessor requires the TokenizeProcessor and MWTProcessor in the pipeline. \n",
    "<br>\n",
    "More information on the POS tags: https://universaldependencies.org/u/pos/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "word: Barack\tupos: PROPN\txpos: NNP\n",
      "word: Obama\tupos: PROPN\txpos: NNP\n",
      "word: was\tupos: AUX\txpos: VBD\n",
      "word: born\tupos: VERB\txpos: VBN\n",
      "word: in\tupos: ADP\txpos: IN\n",
      "word: Hawaii\tupos: PROPN\txpos: NNP\n",
      "word: .\tupos: PUNCT\txpos: .\n",
      "====== Sentence 2 tokens =======\n",
      "word: His\tupos: PRON\txpos: PRP$\n",
      "word: style\tupos: NOUN\txpos: NN\n",
      "word: is\tupos: AUX\txpos: VBZ\n",
      "word: different\tupos: ADJ\txpos: JJ\n",
      "word: from\tupos: ADP\txpos: IN\n",
      "word: Donald\tupos: PROPN\txpos: NNP\n",
      "word: Trump's\tupos: PROPN\txpos: NNP\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos', verbose=False)\n",
    "doc = nlp(text)\n",
    "for i, sent in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}' for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LemmaProcessor</b>\n",
    "<br>\n",
    "As other NLP toolkits, Stanza also supports Lemmatisation process, it called `LemmaProcessor`.<br>\n",
    "TokenizeProcessor, MWTProcessor, and POSProcessor are the requisite in the pipeline to run LemmaProcessor. \n",
    "<br>\n",
    "Lemmatizing words in a sentence and accessing their lemmas afterwards can be done as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "word: Barack \tlemma: Barack\n",
      "word: Obama \tlemma: Obama\n",
      "word: was \tlemma: be\n",
      "word: born \tlemma: bear\n",
      "word: in \tlemma: in\n",
      "word: Hawaii \tlemma: Hawaii\n",
      "word: . \tlemma: .\n",
      "====== Sentence 2 tokens =======\n",
      "word: His \tlemma: his\n",
      "word: style \tlemma: style\n",
      "word: is \tlemma: be\n",
      "word: different \tlemma: different\n",
      "word: from \tlemma: from\n",
      "word: Donald \tlemma: Donald\n",
      "word: Trump's \tlemma: Trump'\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma', verbose=False)\n",
    "doc = nlp(text)\n",
    "for i, sent in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DepparseProcessor</b>\n",
    "<br>\n",
    "To check how well you model can understand each word in your full sentence, you can use `DepparseProcessor` which provides an accurate syntactic dependency parser.\n",
    "<br>\n",
    "Remember: DepparseProcessor requiresTokenizeProcessor, MWTProcessor, POSProcessor and LemmaProcessor in the pipeline. The head index of each Word can be accessed by the property `head`, and the dependency relation between the words `deprel` .\n",
    "<br>\n",
    "This is example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: 1\tword: Barack\thead id: 4\thead: born\tdeprel: nsubj:pass\n",
      "id: 2\tword: Obama\thead id: 1\thead: Barack\tdeprel: flat\n",
      "id: 3\tword: was\thead id: 4\thead: born\tdeprel: aux:pass\n",
      "id: 4\tword: born\thead id: 0\thead: root\tdeprel: root\n",
      "id: 5\tword: in\thead id: 6\thead: Hawaii\tdeprel: case\n",
      "id: 6\tword: Hawaii\thead id: 4\thead: born\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 4\thead: born\tdeprel: punct\n",
      "====== Sentence 2 tokens =======\n",
      "id: 1\tword: His\thead id: 2\thead: style\tdeprel: nmod:poss\n",
      "id: 2\tword: style\thead id: 4\thead: different\tdeprel: nsubj\n",
      "id: 3\tword: is\thead id: 4\thead: different\tdeprel: cop\n",
      "id: 4\tword: different\thead id: 0\thead: root\tdeprel: root\n",
      "id: 5\tword: from\thead id: 6\thead: Donald\tdeprel: case\n",
      "id: 6\tword: Donald\thead id: 4\thead: different\tdeprel: obl\n",
      "id: 7\tword: Trump's\thead id: 6\thead: Donald\tdeprel: flat\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse', verbose = False)\n",
    "doc = nlp(text)\n",
    "for i, sent in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}'  for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a better visualization the result, you can install `spacy_stanza` package. This package wraps the Stanza library, so you can use the display api from spacy to render the result like this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name Entity Recognition</b>\n",
    "<br>\n",
    "In Stanza, NER is performed by the NERProcessor and can be invoked by the name `ner`. NER must be used together with the tokenizer in the process pipeline. For the moment, this is only supported for 8 out of the 66 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Barack Obama\ttype: PERSON\n",
      "entity: Hawaii\ttype: GPE\n",
      "entity: Donald Trump's\ttype: PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner', verbose = False)\n",
    "\n",
    "doc = nlp(text)\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10+"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
